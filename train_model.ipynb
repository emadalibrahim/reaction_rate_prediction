{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training file\n",
    "This file runs training for a given file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from ipynb.fs.full.Utils import arrhenius\n",
    "from ipynb.fs.full.Data_loader import map_to_scale\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,optimizer,criterion,n_epochs=100,weight=[1,1,1,1],bweight=[1,1,1,1],train_loader=None, scheduler=None, valid_loader=None, scaler=None):\n",
    "        \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    losses = {'train':[],'valid':[]}\n",
    "    valid_loss_min = np.Inf\n",
    "    train_loss_min = np.Inf\n",
    "    \n",
    "    try:\n",
    "        model.fck # check if model outputs k, if not skip to Arrhenius loss\n",
    "        \n",
    "        for epoch in range(1, n_epochs+1):\n",
    "\n",
    "            # keep track of training and validation loss\n",
    "            train_loss = 0.0\n",
    "            valid_loss = 0.0\n",
    "            ###################\n",
    "            # train the model #\n",
    "            ###################\n",
    "            model.train()\n",
    "            for features, target, temp in train_loader:\n",
    "                features = features.to(device)\n",
    "                target = target.to(device)\n",
    "                temp   = temp.to(device)\n",
    "                temp = temp.reshape([temp.shape[0],1]);\n",
    "                if model.fck.out_features==1:\n",
    "                    targetlogk=target[:,0].reshape([target.shape[0],1])\n",
    "                else:\n",
    "                    targetlogk=target[:,[0,4,8,12]]\n",
    "                data = torch.cat((features,temp),1)\n",
    "                optimizer.zero_grad()\n",
    "                # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                logk = model(data)\n",
    "                # calculate the batch loss\n",
    "                loss = calculate_loss_simple(targetlogk,logk,criterion)\n",
    "                # backward pass: compute gradient of the loss with respect to model parameters\n",
    "                loss.backward()\n",
    "                # perform a single optimization step (parameter update)\n",
    "                optimizer.step()    \n",
    "                # update training loss\n",
    "                train_loss += loss.item()*data.size(0)\n",
    "            \n",
    "            model.eval()\n",
    "            for features, target, temp in valid_loader:\n",
    "                features = features.to(device)\n",
    "                target = target.to(device)\n",
    "                temp   = temp.to(device)\n",
    "                temp = temp.reshape([temp.shape[0],1])\n",
    "                if model.fck.out_features==1:\n",
    "                    targetlogk=target[:,0].reshape([target.shape[0],1])\n",
    "                else:\n",
    "                    targetlogk=target[:,[0,4,8,12]]\n",
    "                data = torch.cat((features,temp),1)\n",
    "                # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                logk = model(data)\n",
    "                # calculate the batch loss\n",
    "#                 loss = calculate_loss(scaler.torch_inverse(targetlogk),scaler.torch_inverse(logk),criterion,bweight)\n",
    "                loss = calculate_loss_simple(targetlogk,logk,criterion)\n",
    "                valid_loss += loss.item()*data.size(0)\n",
    "                \n",
    "            scheduler.step()\n",
    "            # calculate average losses\n",
    "            train_loss = train_loss/len(train_loader.dataset)\n",
    "            losses['train'].append(train_loss)\n",
    "            valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "            losses['valid'].append(valid_loss)\n",
    "            # print training statistics \n",
    "            if epoch%10 == 0 or epoch ==1:\n",
    "                print('Epoch: {} \\tTraining Loss: MSE = {:.6f} ; RMSE = {:.6f}'.format(\n",
    "                    epoch, train_loss, np.sqrt(train_loss)))\n",
    "                if not (valid_loader==None):\n",
    "                    print('\\t\\tValidation Loss: MSE = {:.6f} ; RMSE = {:.6f}'.format(\n",
    "                            valid_loss, np.sqrt(valid_loss)))    \n",
    "        \n",
    "    except AttributeError :\n",
    "\n",
    "        for epoch in range(1, n_epochs+1):\n",
    "\n",
    "            # keep track of training and validation loss\n",
    "            train_loss = 0.0\n",
    "            train_lossA = 0.0\n",
    "            train_lossB = 0.0\n",
    "            train_lossn = 0.0\n",
    "            train_lossArrhenius = 0.0\n",
    "            valid_loss = 0.0\n",
    "            valid_lossArrhenius = 0.0\n",
    "            rmse_loss = 0.0\n",
    "            total_notnan = 0\n",
    "\n",
    "            \n",
    "            ###################\n",
    "            # train the model #\n",
    "            ###################\n",
    "            model.train()\n",
    "            for features, target, temp in train_loader:\n",
    "                features = features.to(device)\n",
    "                target = target.to(device)\n",
    "                temp   = temp.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                # forward pass: compute predicted outputs by passing inputs to the mode\n",
    "                A,n,B = model(features)\n",
    "                # calculate the batch loss\n",
    "                lossA,lossn,lossB,lossArrhenius = calculate_arrhenius_loss(target,A,B,n,criterion,temp,model,scaler,bweight)\n",
    "                loss = weight[0]*lossArrhenius+weight[1]*lossA+weight[2]*lossn+weight[3]*lossB \n",
    "                # backward pass: compute gradient of the loss with respect to model parameters\n",
    "                loss.backward()\n",
    "                # perform a single optimization step (parameter update)\n",
    "                optimizer.step()    \n",
    "                # update training loss\n",
    "                train_lossA += lossA.item()*features.size(0)\n",
    "                train_lossB += lossB.item()*features.size(0)\n",
    "                train_lossn += lossn.item()*features.size(0)\n",
    "                train_lossArrhenius += lossArrhenius.item()*features.size(0)\n",
    "                train_loss += loss.item()*features.size(0)\n",
    "\n",
    "            model.eval()\n",
    "            for features, target, temp in valid_loader:\n",
    "                features = features.to(device)\n",
    "                target = target.to(device)\n",
    "                temp   = temp.to(device)\n",
    "                # forward pass: compute predicted outputs by passing inputs to the mode\n",
    "                A,n,B = model(features)\n",
    "                lossA,lossn,lossB,lossArrhenius = calculate_arrhenius_loss(target,A,B,n,criterion,temp,model,scaler,bweight)\n",
    "                loss = weight[0]*lossArrhenius+weight[1]*lossA+weight[2]*lossn+weight[3]*lossB\n",
    "                # update training loss\n",
    "                valid_loss += loss.item()*features.size(0)\n",
    "                valid_lossArrhenius += lossArrhenius.item()*features.size(0)\n",
    "#                 rmse,notnan = calculate_rmse(target,A,B,n,criterion,temp,model,scaler,test_on)\n",
    "#                 total_notnan += notnan\n",
    "#                 rmse_loss += rmse.item()\n",
    "                    \n",
    "            scheduler.step()\n",
    "            # calculate average losses\n",
    "            train_loss = train_loss/len(train_loader.dataset)\n",
    "            train_lossA = train_lossA/len(train_loader.dataset)\n",
    "            train_lossB = train_lossB/len(train_loader.dataset)\n",
    "            train_lossn = train_lossn/len(train_loader.dataset)\n",
    "            train_lossArrhenius = train_lossArrhenius/len(train_loader.dataset)\n",
    "            valid_lossArrhenius = valid_lossArrhenius/len(valid_loader.dataset)\n",
    "#             rmse_loss = (rmse_loss/total_notnan)\n",
    "            losses['train'].append(train_loss)\n",
    "            #losses['trainA'].append(train_lossA)\n",
    "            #losses['trainB'].append(train_lossB)\n",
    "            #losses['trainn'].append(train_lossn)\n",
    "            #losses['trainArrhenius'].append(train_lossArrhenius)\n",
    "            valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "            losses['valid'].append(valid_loss)\n",
    "            # print training statistics\n",
    "            if epoch%10 == 0 or epoch ==1:\n",
    "                print('Epoch: {} \\tTraining Loss: MSE = {:.6f} ; RMSE = {:.6f}'.format(\n",
    "                    epoch, train_loss, np.sqrt(train_loss)))\n",
    "                print('Epoch: {} \\tTraining A Loss: MSE = {:.6f} ; RMSE = {:.6f}'.format(\n",
    "                    epoch, train_lossA, np.sqrt(train_lossA)))\n",
    "                print('Epoch: {} \\tTraining B Loss: MSE = {:.6f} ; RMSE = {:.6f}'.format(\n",
    "                    epoch, train_lossB, np.sqrt(train_lossB)))\n",
    "                print('Epoch: {} \\tTraining n Loss: MSE = {:.6f} ; RMSE = {:.6f}'.format(\n",
    "                    epoch, train_lossn, np.sqrt(train_lossn)))\n",
    "                print('Epoch: {} \\tTraining Arrhenius Loss: MSE = {:.6f} ; RMSE = {:.6f}'.format(\n",
    "                    epoch, train_lossArrhenius, np.sqrt(train_lossArrhenius)))\n",
    "                print('Epoch: {} \\tTraining Arrhenius Loss: MSE = {:.6f} ; RMSE = {:.6f}'.format(\n",
    "                    epoch, valid_lossArrhenius, np.sqrt(valid_lossArrhenius)))\n",
    "#                 print('Epoch: {} \\tTraining Arrhenius Loss: MSE = {:.6f} ; RMSE = {:.6f}'.format(\n",
    "#                     epoch, rmse_loss, np.sqrt(rmse_loss)))\n",
    "    \n",
    "    return(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_loss_simple(target,output,criterion):\n",
    "    loss = torch.nan_to_num(criterion(output[~target[:,0].isnan(),0],target[~target[:,0].isnan(),0]))\n",
    "    for t in range(1,output.shape[1]):\n",
    "        loss += torch.nan_to_num(criterion(output[~target[:,t].isnan(),t],target[~target[:,t].isnan(),t]))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calculate_loss(target,output,criterion,bweight):\n",
    "#     loss = bweight[0]*torch.nan_to_num(criterion(output[~target[:,0].isnan(),0],target[~target[:,0].isnan(),0]))\n",
    "#     for t in range(1,output.shape[1]):\n",
    "#         loss += bweight[t]*torch.nan_to_num(criterion(output[~target[:,t].isnan(),t],target[~target[:,t].isnan(),t]))\n",
    "#     return loss\n",
    "    loss = bweight[0]*torch.nan_to_num(criterion(output[~target[:,0].isnan(),0],target[~target[:,0].isnan(),0])/target[~target[:,0].isnan(),0].shape[0])\n",
    "    for t in range(1,output.shape[1]):\n",
    "        loss += bweight[t]*torch.nan_to_num(criterion(output[~target[:,t].isnan(),t],target[~target[:,t].isnan(),t])/target[~target[:,t].isnan(),t].shape[0])\n",
    "    return loss\n",
    "\n",
    "def calculate_arrhenius_loss(target,A,B,n,criterion,temp,model,scaler,bweight):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Map target to variables\n",
    "    j = np.array(list(range(model.fcA.out_features)))*4\n",
    "    targetlogk = target[:,j]\n",
    "    targetA = target[:,(j+1)]\n",
    "    targetB = target[:,(j+2)]\n",
    "    targetn = target[:,(j+3)]\n",
    "    # Calculate loss for each variable\n",
    "    lossA = calculate_loss(targetA,A,criterion,bweight)\n",
    "    lossn = calculate_loss(targetn,n,criterion,bweight)\n",
    "    lossB = calculate_loss(targetB,B,criterion,bweight)\n",
    "    # Scale transform if needed\n",
    "    if scaler.tscaler != None:\n",
    "        scaled = scaler.torch_inverse(map_to_scale(A,B,n))\n",
    "        j = np.array(list(range(model.fcA.out_features)))*3\n",
    "        A = scaled[:,(j+0)]\n",
    "        B = scaled[:,(j+1)]\n",
    "        n = scaled[:,(j+2)]\n",
    "    # Calculate Arrhenius loss\n",
    "#     lossArrhenius = bweight[0]*torch.nan_to_num(criterion(scaler.torch_transform(0,arrhenius(torch.pow(10.,A[:,0]),n[:,0],B[:,0],temp))[~targetlogk[:,0].isnan()],targetlogk[~targetlogk[:,0].isnan(),0])/targetlogk[~targetlogk[:,0].isnan(),0].shape[0])\n",
    "#     for t in range(1,A.shape[1]):\n",
    "#         lossArrhenius += bweight[t]*torch.nan_to_num(criterion(scaler.torch_transform(t,arrhenius(torch.pow(10.,A[:,t]),n[:,t],B[:,t],temp))[~targetlogk[:,t].isnan()], targetlogk[~targetlogk[:,t].isnan(),t])/targetlogk[~targetlogk[:,t].isnan(),t].shape[0])\n",
    "#     return(lossA,lossn,lossB,lossArrhenius)\n",
    "    lossArrhenius = bweight[0]*torch.nan_to_num(criterion(arrhenius(torch.pow(10.,A[:,0]),n[:,0],B[:,0],temp)[~targetlogk[:,0].isnan()], targetlogk[~targetlogk[:,0].isnan(),0])/targetlogk[~targetlogk[:,0].isnan(),0].shape[0])\n",
    "    for t in range(1,A.shape[1]):\n",
    "        lossArrhenius += bweight[t]*torch.nan_to_num(criterion(arrhenius(torch.pow(10.,A[:,t]),n[:,t],B[:,t],temp)[~targetlogk[:,t].isnan()], targetlogk[~targetlogk[:,t].isnan(),t])/targetlogk[~targetlogk[:,t].isnan(),t].shape[0])\n",
    "    return(lossA,lossn,lossB,lossArrhenius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse(target,A,B,n,criterion,temp,model,scaler,test_on):\n",
    "    bweight=[1,1,1,1]\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Map target to variables\n",
    "    j = np.array(list(range(model.fcA.out_features)))*4\n",
    "    targetlogk = target[:,j]\n",
    "    targetA = target[:,(j+1)]\n",
    "    targetB = target[:,(j+2)]\n",
    "    targetn = target[:,(j+3)]\n",
    "    # Calculate loss for each variable\n",
    "    # Scale transform if needed\n",
    "    if scaler.tscaler != None:\n",
    "        scaled = scaler.torch_inverse(map_to_scale(A,B,n))\n",
    "        j = np.array(list(range(model.fcA.out_features)))*3\n",
    "        A = scaled[:,(j+0)]\n",
    "        B = scaled[:,(j+1)]\n",
    "        n = scaled[:,(j+2)]\n",
    "    # Calculate Arrhenius loss\n",
    "#     lossArrhenius = bweight[0]*torch.nan_to_num(criterion(scaler.torch_transform(0,arrhenius(torch.pow(10.,A[:,0]),n[:,0],B[:,0],temp)[~targetlogk[:,0].isnan()]),targetlogk[~targetlogk[:,0].isnan(),0]))\n",
    "#     for t in range(1,A.shape[1]):\n",
    "#         lossArrhenius += bweight[t]*torch.nan_to_num(criterion(scaler.torch_transform(t,arrhenius(torch.pow(10.,A[:,t]),n[:,t],B[:,t],temp)[~targetlogk[:,t].isnan()]), targetlogk[~targetlogk[:,t].isnan(),t]))\n",
    "#     lossArrhenius = bweight[0]*torch.nan_to_num(criterion(arrhenius(torch.pow(10.,A[:,0]),n[:,0],B[:,0],temp)[~targetlogk[:,0].isnan()],targetlogk[~targetlogk[:,0].isnan(),0]))\n",
    "#     for t in range(1,A.shape[1]):\n",
    "#         lossArrhenius += bweight[t]*torch.nan_to_num(criterion(arrhenius(torch.pow(10.,A[:,t]),n[:,t],B[:,t],temp)[~targetlogk[:,t].isnan()], targetlogk[~targetlogk[:,t].isnan(),t]))\n",
    "    print(model.fcA.out_features,test_on)\n",
    "    if model.fcA.out_features==4 and test_on=='OH':\n",
    "        t=0\n",
    "    elif model.fcA.out_features==4 and test_on=='O3':\n",
    "        t=1\n",
    "    elif model.fcA.out_features==4 and test_on=='NO3':\n",
    "        t=2\n",
    "    elif model.fcA.out_features==4 and test_on=='Cl':\n",
    "        t=3\n",
    "    else:\n",
    "        t=0\n",
    "    \n",
    "    print(t)\n",
    "#     lossArrhenius = torch.nan_to_num(criterion(arrhenius(torch.pow(10.,A[:,t]),n[:,t],B[:,t],temp)[~targetlogk[:,t].isnan()],scaler.torch_inverse2(t,targetlogk[~targetlogk[:,t].isnan(),t]))*targetlogk[~targetlogk[:,t].isnan(),t].shape[0])\n",
    "    lossArrhenius = torch.nan_to_num(criterion(arrhenius(torch.pow(10.,A[:,t]),n[:,t],B[:,t],temp)[~targetlogk[:,t].isnan()],targetlogk[~targetlogk[:,t].isnan(),t])*targetlogk[~targetlogk[:,t].isnan(),t].shape[0])\n",
    "    return lossArrhenius,targetlogk[~targetlogk[:,t].isnan(),t].shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "chemprop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
