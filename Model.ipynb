{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model file\n",
    "Defines and returns model architecture. Takes in dataframe, input type ('Chemprop','Morgan', or 'All' for both), n_hidden (number of hidden nodes), and model type ('Arrhenius','Point')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(df,param):\n",
    "    # Extract length of fingerprints\n",
    "    n_cp =(df.columns.get_loc(\"fp_1399\")-df.columns.get_loc(\"fp_0\"))+1\n",
    "    n_morgan = df.columns.get_loc(2047)-df.columns.get_loc(0)\n",
    "    # Determine input size\n",
    "    if param.input_type == 'All':\n",
    "        n_input = n_cp + n_morgan\n",
    "    elif param.input_type == 'Chemprop':\n",
    "        n_input = n_cp\n",
    "    elif param.input_type == 'Morgan':\n",
    "        n_input = n_morgan\n",
    "    else:\n",
    "        print('Input type not supported')\n",
    "    # Define specified model\n",
    "    if param.model_type == 'Point' :\n",
    "        class model(nn.Module):\n",
    "            def __init__(self,p=param.dropout):\n",
    "                super(model, self).__init__()\n",
    "                self.drop_layer = nn.Dropout(p=p)\n",
    "                if param.batch_norm:\n",
    "                    self.fc1 = nn.Linear(n_input+1, param.hidden[0], bias=False) # Add one for Temperature\n",
    "                    self.bn = nn.BatchNorm1d(param.hidden[0])\n",
    "                else:\n",
    "                    self.fc1 = nn.Linear(n_input+1, param.hidden[0])\n",
    "                modules = []\n",
    "                for i in range(len(param.hidden)-1):\n",
    "                    if param.batch_norm:\n",
    "                    # append batchnorm layer\n",
    "                        modules.append(nn.Dropout(p=p))\n",
    "                        modules.append(nn.Linear(param.hidden[i], param.hidden[i+1], bias=False))\n",
    "                        modules.append(nn.BatchNorm1d(param.hidden[i+1]))\n",
    "                    else:\n",
    "                        modules.append(nn.Dropout(p=p))\n",
    "                        modules.append(nn.Linear(param.hidden[i], param.hidden[i+1]))\n",
    "                    modules.append(nn.ReLU())\n",
    "                self.body = nn.Sequential(*modules)\n",
    "                if param.target == 'All':\n",
    "                    self.fck = nn.Linear(param.hidden[-1], 4)\n",
    "                else:\n",
    "                    self.fck = nn.Linear(param.hidden[-1], 1)\n",
    "            \n",
    "            def forward(self, x):\n",
    "                x = self.drop_layer(x)\n",
    "                if param.batch_norm:\n",
    "                    x = F.relu_(self.bn(self.fc1(x)))\n",
    "                else:\n",
    "                    x = F.relu_(self.fc1(x))\n",
    "                x = self.body(x)\n",
    "                k = self.fck(x)\n",
    "                return k\n",
    "            \n",
    "    elif param.model_type == 'Arrhenius' :\n",
    "        class model(nn.Module):\n",
    "            def __init__(self,p=param.dropout):\n",
    "                super(model, self).__init__()\n",
    "                self.drop_layer = nn.Dropout(p=p)\n",
    "                if param.batch_norm:\n",
    "                    self.fc1 = nn.Linear(n_input, param.hidden[0], bias=False)\n",
    "                    self.bn = nn.BatchNorm1d(param.hidden[0])\n",
    "                else:\n",
    "                    self.fc1 = nn.Linear(n_input, param.hidden[0], bias=True)\n",
    "                modules = []\n",
    "                for i in range(len(param.hidden)-1):\n",
    "                    if param.batch_norm:\n",
    "                    # append batchnorm layer\n",
    "                        modules.append(nn.Dropout(p=p))\n",
    "                        modules.append(nn.Linear(param.hidden[i], param.hidden[i+1], bias=False))\n",
    "                        modules.append(nn.BatchNorm1d(param.hidden[i+1]))\n",
    "                    else:\n",
    "                        modules.append(nn.Dropout(p=p))\n",
    "                        modules.append(nn.Linear(param.hidden[i], param.hidden[i+1]))\n",
    "                    modules.append(nn.ReLU())\n",
    "                self.body = nn.Sequential(*modules)\n",
    "                if param.target == 'All':\n",
    "                    self.fcA = nn.Linear(param.hidden[-1], 4)\n",
    "                    self.fcn = nn.Linear(param.hidden[-1], 4)\n",
    "                    self.fcB = nn.Linear(param.hidden[-1], 4)\n",
    "#                     self.fcA = nn.Linear(param.hidden[-1]+12, 4)\n",
    "#                     self.fcn = nn.Linear(param.hidden[-1]+12, 4)\n",
    "#                     self.fcB = nn.Linear(param.hidden[-1]+12, 4)\n",
    "                else:\n",
    "                    self.fcA = nn.Linear(param.hidden[-1], 1)\n",
    "                    self.fcn = nn.Linear(param.hidden[-1], 1)\n",
    "                    self.fcB = nn.Linear(param.hidden[-1], 1)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = self.drop_layer(x)\n",
    "                if param.batch_norm:\n",
    "                    x = F.relu_(self.bn(self.fc1(x)))\n",
    "                else:\n",
    "                    x = F.relu_(self.fc1(x))\n",
    "                x = self.body(x)\n",
    "                A = self.fcA(x)\n",
    "                n = self.fcn(x)\n",
    "                B = self.fcB(x)\n",
    "#                 A0 = self.fcA0(self.body1(x))\n",
    "#                 n0 = self.fcn0(self.body2(x))\n",
    "#                 B0 = self.fcB0(self.body3(x))\n",
    "#                 A = self.fcA(torch.cat((x,A0,n0,B0),dim=1))\n",
    "#                 n = self.fcn(torch.cat((x,A0,n0,B0),dim=1))\n",
    "#                 B = self.fcB(torch.cat((x,A0,n0,B0),dim=1))\n",
    "                \n",
    "                return A,n,B\n",
    "    else:\n",
    "        print('Input type not supported')\n",
    "    \n",
    "    model = model()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "        model = nn.DataParallel(model).module\n",
    "    model.to(device)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wrapper function to define model, optimizer, criterion,scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_initialize(df,param):\n",
    "    model = build_model(df,param)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=param.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=param.scheduler_step,gamma=param.scheduler_gamma)\n",
    "    return(model, criterion, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "chemprop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
